{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302f36b7-0e7d-4647-85f5-6a7e197d6cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ec97ed-d7c7-44a4-9324-db2575828660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA Setup\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "#from torch.optim import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# Basic setup\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize tokenizer and constants\n",
    "MAX_SEQ_LEN = 128\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class HateData(Dataset):\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __init__(self, data_path, split='train', aug_prob=0.2, flip_prob=0.5):\n",
    "        self.split = split\n",
    "        self.data = pd.read_csv(data_path, sep='\\t', lineterminator='\\n')\n",
    "        \n",
    "        # Convert 3-class to 2-class format\n",
    "        self.data['binary_label'] = self.data['class'].apply(lambda x: 1 if x == 1 else 0)\n",
    "        \n",
    "        print(f\"\\nLoading {split} data:\")\n",
    "        print(\"Original label distribution:\")\n",
    "        # print(self.data['label'].value_counts())\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.label2data = {0:[], 1:[]}\n",
    "            print(\"Creating binary label pools...\")\n",
    "            for i in tqdm(range(len(self.data))):\n",
    "                row = self.data.iloc[i]\n",
    "                self.label2data[row['binary_label']].append(row['post'])\n",
    "            self.aug_prob = aug_prob\n",
    "            self.flip_prob = flip_prob\n",
    "            \n",
    "            # Print label distribution\n",
    "            print(\"\\nOriginal label distribution:\")\n",
    "            # print(self.data['label'].value_counts())\n",
    "            print(\"\\nBinary label distribution:\")\n",
    "            print(self.data['binary_label'].value_counts())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        data = self.data.iloc[index]\n",
    "        labels = data['binary_label']  # Use binary labels\n",
    "        text = data['post']\n",
    "        \n",
    "        inputs = tokenizer(text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask = inputs['attention_mask']\n",
    "        \n",
    "        aug_text = text  \n",
    "        labels_aug = labels\n",
    "        \n",
    "        if self.split == 'train' and labels == 1:\n",
    "            if np.random.uniform() < self.aug_prob:\n",
    "                aug_text = np.random.choice(self.label2data[0])\n",
    "                \n",
    "                if np.random.uniform() < self.flip_prob:\n",
    "                    aug_text = aug_text + \" [SEP] \" + text\n",
    "                else:\n",
    "                    aug_text = text + \" [SEP] \" + aug_text\n",
    "                labels_aug = 1\n",
    "        \n",
    "        inputs_aug = tokenizer(aug_text, padding='max_length', truncation=True, max_length=MAX_SEQ_LEN)\n",
    "        input_ids_aug = inputs_aug['input_ids']\n",
    "        token_type_ids_aug = np.zeros(MAX_SEQ_LEN)\n",
    "        attn_mask_aug = inputs_aug['attention_mask']\n",
    "\n",
    "        input_ids = torch.tensor(np.vstack([input_ids, input_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        token_type_ids = torch.tensor(np.vstack([token_type_ids, token_type_ids_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        attn_mask = torch.tensor(np.vstack([attn_mask, attn_mask_aug]), dtype=torch.long).view(2, MAX_SEQ_LEN)\n",
    "        labels = torch.tensor(np.vstack([labels, labels_aug]), dtype=torch.long).view(2)\n",
    "\n",
    "        return input_ids, attn_mask, token_type_ids, labels\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        H1, H2 = 768, 128  # Hidden layer sizes\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "        \n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(H1, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, H2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H2, 2)  # Binary output\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attn_mask)\n",
    "        cls_emb = outputs.pooler_output\n",
    "        logits = self.clf(cls_emb)\n",
    "        return logits\n",
    "\n",
    "def train(input_ids, attn_mask, token_type_ids, label, model, model_opt, scdl):\n",
    "    model_opt.zero_grad()\n",
    "\n",
    "    if use_cuda:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "    # Debugging: Print shapes and types\n",
    "    # print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "    # print(f\"Attention Mask shape: {attn_mask.shape}\")\n",
    "    # print(f\"Token Type IDs shape: {token_type_ids.shape}\")\n",
    "    # print(f\"Labels shape: {label.shape}\")\n",
    "    # print(f\"Input IDs dtype: {input_ids.dtype}\")\n",
    "    # print(f\"Labels dtype: {label.dtype}\")\n",
    "    # print(f\"Unique labels: {torch.unique(label)}\")\n",
    "    \n",
    "    # Get model outputs\n",
    "    logits = model(input_ids[:, 0, :], attn_mask[:, 0, :], token_type_ids[:, 0, :])\n",
    "    logits_aug = model(input_ids[:, 1, :], attn_mask[:, 1, :], token_type_ids[:, 1, :])\n",
    "    \n",
    "    # Debugging: Print logits shape\n",
    "    # print(f\"Logits shape: {logits.shape}\")\n",
    "    \n",
    "    # Convert labels to binary (0: non-hate, 1: hate)\n",
    "    binary_labels = torch.where(label == 1, 1, 0)\n",
    "    \n",
    "    # Debugging: Print binary labels and their type\n",
    "    # print(f\"Binary Labels: {binary_labels}\")\n",
    "    # print(f\"Binary Labels dtype: {binary_labels.dtype}\")\n",
    "    \n",
    "    # Calculate loss with binary labels\n",
    "    loss = loss_fn(logits, binary_labels[:, 0]) + loss_fn(logits_aug, binary_labels[:, 1])\n",
    "    \n",
    "    loss.backward()\n",
    "    model_opt.step()\n",
    "    scdl.step()\n",
    "    \n",
    "    return float(loss.item())\n",
    "\n",
    "def evaluate(input_ids, attn_mask, token_type_ids, label, model, mode='train'):\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "        # Convert labels to binary\n",
    "        binary_labels = torch.where(label == 1, 1, 0)\n",
    "        \n",
    "        logits = model(input_ids[:, 0, :], attn_mask[:, 0, :], token_type_ids[:, 0, :])\n",
    "        loss = loss_fn(logits, binary_labels[:, 0])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return float(loss.item())\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        return float(loss.item()), preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5353def-06f5-498d-a78a-b8940b3dc82b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HateData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_total\u001b[38;5;241m/\u001b[39mtot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load your train and test data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mHateData\u001b[49m(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/hatexplain/hx_train.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m HateData(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/hatexplain/hx_test.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HateData' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load your train and test data\n",
    "    train_data = HateData(data_path=\"data/hatexplain/hx_train.tsv\", split='train')\n",
    "    val_data = HateData(data_path=\"data/hatexplain/hx_test.tsv\", split='test')\n",
    "    \n",
    "    # Create dataloaders\n",
    "    BS = 16\n",
    "    train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    # Load test labels\n",
    "    df_test = pd.read_csv(\"data/hatexplain/hx_test.tsv\", sep='\\t', lineterminator='\\n')\n",
    "    global gt_labels\n",
    "    gt_labels = np.array([1 if label == 1 else 0 for label in df_test['label']])\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Classifier().to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "    num_training_steps = len(train_loader) * 5  # 5 epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * num_training_steps),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    print(\"Initialized optimizer and lr scheduler\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    tot = len(train_data) // train_loader.batch_size\n",
    "    tot_val = len(val_data) // val_loader.batch_size\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_step = 0\n",
    "        print(f'Epoch: {epoch}')\n",
    "        progress_bar = tqdm(train_loader, total=tot, position=0, leave=True)\n",
    "        for entry in progress_bar:\n",
    "            loss = train(entry[0], entry[1], entry[2], entry[3], model, optimizer, scheduler)\n",
    "            train_step += 1\n",
    "            train_loss_total += loss\n",
    "            \n",
    "            train_loss = train_loss_total / train_step\n",
    "            progress_bar.set_postfix({'loss': train_loss})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        \n",
    "        for entry in tqdm(val_loader, total=tot_val, position=0, leave=True):\n",
    "            loss_v, pred_v = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "            test_pred.extend([pd for pd in pred_v])\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        val_f1 = f1_score(gt_labels, test_pred, average='macro')\n",
    "        val_acc = accuracy_score(gt_labels, test_pred)\n",
    "        print(f\"\\nValidation F1 Score: {val_f1:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # print(classification_report(gt_labels, test_pred, digits=4))\n",
    "        \n",
    "        if val_acc > best_f1:\n",
    "            torch.save(model.state_dict(), \"best_model_roberta_hatexplain_easymix.pth\")\n",
    "            print(\"Model saved\")\n",
    "            best_f1 = val_acc\n",
    "        \n",
    "        \n",
    "        print(f'Total loss: {train_loss_total/tot:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d00798-54e0-41ed-883e-e1b46df3c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data:\n",
      "Original label distribution:\n",
      "Creating binary label pools...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14273/14273 [00:01<00:00, 12351.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original label distribution:\n",
      "\n",
      "Binary label distribution:\n",
      "0    9286\n",
      "1    4987\n",
      "Name: binary_label, dtype: int64\n",
      "\n",
      "Loading test data:\n",
      "Original label distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized optimizer and lr scheduler\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "893it [05:41,  2.62it/s, loss=1.19]                         \n",
      "255it [00:18, 14.10it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation F1 Score: 0.6993\n",
      "Validation Accuracy: 0.7124\n",
      "Model saved\n",
      "Total loss: 1.1921\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "893it [05:40,  2.62it/s, loss=0.947]                         \n",
      "255it [00:17, 14.20it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation F1 Score: 0.7384\n",
      "Validation Accuracy: 0.7639\n",
      "Model saved\n",
      "Total loss: 0.9485\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 614/892 [03:54<01:45,  2.63it/s, loss=0.798]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load your train and test data\n",
    "    train_data = HateData(data_path=\"data/latenthatred/latent_train.tsv\", split='train')\n",
    "    val_data = HateData(data_path=\"data/latenthatred/latent_test.tsv\", split='test')\n",
    "    \n",
    "    # Create dataloaders\n",
    "    BS = 16\n",
    "    train_loader = DataLoader(train_data, batch_size=BS, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    # Load test labels\n",
    "    df_test = pd.read_csv(\"data/latenthatred/latent_test.tsv\", sep='\\t', lineterminator='\\n')\n",
    "    global gt_labels\n",
    "    gt_labels = np.array([1 if label == 1 else 0 for label in df_test['class']])\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Classifier().to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "    num_training_steps = len(train_loader) * 5  # 5 epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * num_training_steps),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    print(\"Initialized optimizer and lr scheduler\")\n",
    "    \n",
    "    best_f1 = 0\n",
    "    tot = len(train_data) // train_loader.batch_size\n",
    "    tot_val = len(val_data) // val_loader.batch_size\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_step = 0\n",
    "        print(f'Epoch: {epoch}')\n",
    "        progress_bar = tqdm(train_loader, total=tot, position=0, leave=True)\n",
    "        for entry in progress_bar:\n",
    "            loss = train(entry[0], entry[1], entry[2], entry[3], model, optimizer, scheduler)\n",
    "            train_step += 1\n",
    "            train_loss_total += loss\n",
    "            \n",
    "            train_loss = train_loss_total / train_step\n",
    "            progress_bar.set_postfix({'loss': train_loss})\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        \n",
    "        for entry in tqdm(val_loader, total=tot_val, position=0, leave=True):\n",
    "            loss_v, pred_v = evaluate(entry[0], entry[1], entry[2], entry[3], model, mode='test')\n",
    "            test_pred.extend([pd for pd in pred_v])\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        val_f1 = f1_score(gt_labels, test_pred, average='macro')\n",
    "        val_acc = accuracy_score(gt_labels, test_pred)\n",
    "        print(f\"\\nValidation F1 Score: {val_f1:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # print(classification_report(gt_labels, test_pred, digits=4))\n",
    "        \n",
    "        if val_acc > best_f1:\n",
    "            torch.save(model.state_dict(), \"best_model_roberta_hatexplain_easymix.pth\")\n",
    "            print(\"Model saved\")\n",
    "            best_f1 = val_acc\n",
    "        \n",
    "        \n",
    "        print(f'Total loss: {train_loss_total/tot:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
